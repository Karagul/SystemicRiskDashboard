jaccard[j,i] = jaccard[i,j]
}
}
similarity_score = rowSums(jaccard)
}
medianJaccard = function(text, n){
jaccardRow = calculateJaccard(text, n)
median(jaccardRow)
}
medianJaccard(storyT,length(storyT))
a
calculateJaccard = function(text, n){
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,m)  #Store match index
for (i in 1:m) {
for (j in i:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
similarity_score = rowSums(jaccard)
median(similarity_score)
}
storyT
medianJacc = calculateJaccard(storyT,length(storyT))
print(medianJacc)
mText
htmlpage <- read_html("https://www.sec.gov/Archives/edgar/data/1490281/000144530512000922/groupon10-k.htm#s510FC9A31D5A3D1BAD2560B662E0F3F9")
overview <- htmlpage%>%
html_node("div:nth-child(785)")%>%
html_text()
p2 <- htmlpage%>%
html_node("div:nth-child(786)")%>%
html_text()
p3 <- htmlpage%>%
html_node("div:nth-child(787)")%>%
html_text()
p4 <- htmlpage%>%
html_node("div:nth-child(788)")%>%
html_text()
text = c(overview,p2,p3,p4)
text = paste(text,collapse=" ")
text
text2 = strsplit(text,". ",fixed=TRUE)
print(text2)
mText = text2[[1]]
mText
medianJacc = calculateJaccard(storyT[[1]],length(storyT))
print(medianJacc)
storyT
storySentences = strsplit(storyT,". ",fixed=TRUE)
calculateJaccard = function(text, n){
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,m)  #Store match index
for (i in 1:m) {
for (j in i:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
similarity_score = rowSums(jaccard)
median(similarity_score)
}
medianJacc = calculateJaccard(storySentences[[1]],length(storyT))
print(medianJacc)
medianJacc = calculateJaccard(storySentences[[1]],length(storyT))
storySentences = strsplit(storyT,". ",fixed=TRUE)
storyT = plain.text
class(storyT)
storyT = storyT[6:108]
storyT = paste(storyT, collapse = "\n")
gsub("[\r\n]", "", storyT)
storySentences = strsplit(storyT,". ",fixed=TRUE)
medianJacc = calculateJaccard(storySentences[[1]],length(storySentences))
length(storySentences)
length(storySentences[[1]])
medianJacc = calculateJaccard(storySentences[[1]],length(storySentences[[1]]))
print(medianJacc)
plain.text
storyT = plain.text
class(storyT)
storyT = storyT[6:108]
storyT
storyT = plain.text
class(storyT)
#Extract the useful parts of text by removing aurthor name and dates
storyT = storyT[6:108]
storyCorpus = Corpus(VectorSource(storyT))
storyCorpus = tm_map(storyCorpus,removePunctuation)
storyCorpus = tm_map(storyCorpus,removeWords, stopwords("english"))
storyCorpus = tm_map(storyCorpus,content_transformer(tolower))
storyCorpus
as.array(storyCorpus)
txt = NULL
for (j in 1:length(storyCorpus)) {
txt = c(txt,storyCorpus[[j]]$content)
}
txt
storyT = txt
storyT = paste(storyT, collapse = "\n")
gsub("[\r\n]", "", storyT)
storyCorpus = Corpus(VectorSource(storyT))
storyCorpus = tm_map(storyCorpus,removeWords, stopwords("english"))
storyCorpus = tm_map(storyCorpus,content_transformer(tolower))
txt = NULL
for (j in 1:length(storyCorpus)) {
txt = c(txt,storyCorpus[[j]]$content)
}
storyT = txt
storyT = paste(storyT, collapse = "\n")
gsub("[\r\n]", "", storyT)
storyT = plain.text
class(storyT)
#Extract the useful parts of text by removing aurthor name and dates
storyT = storyT[6:108]
storyT = paste(storyT, collapse = "\n")
gsub("[\r\n]", "", storyT)
storySentences = strsplit(storyT,". ",fixed=TRUE)
storySentences
calculateJaccard = function(text, n){
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,m)  #Store match index
for (i in 1:m) {
for (j in i:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
jaccard
}
jaccardM = calculateJaccard(storySentences[[1]],length(storySentences[[1]]))
similarity_score = rowSums(jaccard)
median(similarity_score)
similarity_score = rowSums(jaccardM)
median(similarity_score)
n
n = length(storySentences)
dim(jaccardM)
n
n = length(storySentences[[1]])
n
A = matrix(nc=3, byrow=TRUE, c(0,1,1, 1,0,1, 1,1,0))
print(A)
g = graph.adjacency(A,mode="undirected",weighted=TRUE,diag=FALSE)
library(igraph)
A = matrix(nc=3, byrow=TRUE, c(0,1,1, 1,0,1, 1,1,0))
print(A)
g = graph.adjacency(A,mode="undirected",weighted=TRUE,diag=FALSE)
plot(g)
medianJacc = median(similarity_score)
print(medianJacc)
adjacencyM = function(jaccardMat,n,median){
m = n
adjMat = matrix(0,m,n)
for(i in 1:n){
for(j in i:m){
if(jaccardMat[i,j]>median){
adjMat[i,j] = 1
adjMat[j,i] = 1
}
}
}
adjMat
}
adjMat = adjacencyM(jaccardM,n,medianJacc)
g = graph.adjacency(adjMat,mode="undirected",weighted=TRUE,diag=FALSE)
plot(g)
A = matrix(nc=3, byrow=TRUE, c(0,1,1, 1,0,1, 1,1,0))
print(A)
g = graph.adjacency(A,mode="undirected",weighted=TRUE,diag=FALSE)
plot(g)
res = evcent(g)
print(names(res))
res
el <- matrix(nc=3, byrow=TRUE,c(0,1,0, 0,2,2, 0,3,1, 1,2,0, 1,4,5, 1,5,2, 2,1,1, 2,3,1, 2,6,1, 3,2,0,
3,6,2, 4,5,2, 4,7,8, 5,2,2, 5,6,1, 5,8,1, 5,9,3, 7,5,1, 7,8,1, 8,9,4) )
el[,1:2] = el[,1:2]+1
g = add.edges(graph.empty(10), t(el[,1:2]), weight=el[,3])
plot(g)
adjMat
rowSums(adjMat)
jaccardM
median(jaccardM)
medianJacc = median(jaccardM)
net=graph.adjacency(adjmatrix=adjMat,mode="undirected",weighted=TRUE,diag=FALSE)
adjMat = adjacencyM(jaccardM,n,medianJacc)
net=graph.adjacency(adjmatrix=adjMat,mode="undirected",weighted=TRUE,diag=FALSE)
plot(net)
adjMat
plot(net)
rowSums(adjMat)
adjMat
adjMat[48,]
plot(net)
jaccardM[48,]
calculateJaccard = function(text, n){
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,m)  #Store match index
for (i in 1:m) {
for (j in i+1:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
jaccard
}
#create jaccard matrix using calculate function
jaccardM = calculateJaccard(storySentences[[1]],length(storySentences[[1]]))
similarity_score = rowSums(jaccardM)
jaccardM = calculateJaccard(storySentences[[1]],length(storySentences[[1]]))
adjacencyM = function(jaccardMat,n,median){
m = n
adjMat = matrix(0,m,n)
for(i in 1:n){
for(j in i:m){
if(i!=j & jaccardMat[i,j]>median){
adjMat[i,j] = 1
adjMat[j,i] = 1
}
}
}
adjMat
}
adjMat = adjacencyM(jaccardM,n,medianJacc)
net=graph.adjacency(adjmatrix=adjMat,mode="undirected",weighted=TRUE,diag=FALSE)
plot(net)
rowSums(adjMat[48,])
adjMat[48,]
rowSums(adjMat)
res = evcent(g)
res
centR = evcent(net)
centR
centrVec = sort(centR$vector, index.return=TRUE,
decreasing=TRUE)
centrVec
idx = centrVec$ix[1:n]
summary = storySentences[[1]][idx]
summary
summary[1:10]
storyT = paste(storyT, collapse = "\n")
gsub("[\r\n]", "", storyT)
storySentences = strsplit(storyT,". ",fixed=TRUE)[[1]]
storySentences
jaccardM = calculateJaccard(storySentences[[1]],length(storySentences))
similarity_score = rowSums(jaccardM)
jaccardM = calculateJaccard(storySentences,length(storySentences))
similarity_score = rowSums(jaccardM)
length(storySentences)
calculateJaccard = function(text, n){
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,n)  #Store match index
for (i in 1:m) {
for (j in i:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
jaccard
}
jaccardM = calculateJaccard(storySentences,length(storySentences))
similarity_score = rowSums(jaccardM)
medianJacc = median(jaccardM)
print(medianJacc)
n = length(storySentences)
n
jaccardM = calculateJaccard(storySentences,n)
similarity_score = rowSums(jaccardM)
#Here we find median over entire matrix as we want to find the median jaccard similarity score between 2 sentences
medianJacc = median(jaccardM)
print(medianJacc)
adjacencyM = function(jaccardMat,n,median){
m = n
adjMat = matrix(0,m,n)
for(i in 1:n){
for(j in i:m){
if(i!=j & jaccardMat[i,j]>median){
adjMat[i,j] = 1
adjMat[j,i] = 1
}
}
}
adjMat
}
adjMat = adjacencyM(jaccardM,n,medianJacc)
net=graph.adjacency(adjmatrix=adjMat,mode="undirected",weighted=TRUE,diag=FALSE)
plot(net)
centR = evcent(net)
centR
centrVec = sort(centR$vector, index.return=TRUE,
decreasing=TRUE)
idx = centrVec$ix[1:n]
summary = storySentences[[1]][idx]
summary[1:10]
summary = storySentences[idx]
summary[1:10]
oneTenth = as.integer(n/10)
oneTenth
summary[1:oneTenth]
idx
storySentences[224]
centR
storyCorpus = Corpus(VectorSource(storySentences))
storyCorpus = tm_map(storyCorpus,removePunctuation)
storyCorpus = tm_map(storyCorpus,removeWords, stopwords("english"))
storyCorpus = tm_map(storyCorpus,content_transformer(tolower))
txt = NULL
for (j in 1:length(storyCorpus)) {
txt = c(txt,storyCorpus[[j]]$content)
}
storySentences2 = txt
dim(storySentences2)
storySentences2
length(storySentences2)
medianJacc2
jaccardM2 = calculateJaccard(storySentences2,n)
medianJacc2 = median(jaccardM2)
print(medianJacc2)
adjMat2 = adjacencyM(jaccardM,n,medianJacc)
net2=graph.adjacency(adjmatrix=adjMat2,mode="undirected",weighted=TRUE,diag=FALSE)
plot(net2)
centR2 = evcent(net2)
centR2
centrVec2 = sort(centR2$vector, index.return=TRUE,
decreasing=TRUE)
idx2 = centrVec2$ix[1:n]
summary2 = storySentences2[idx2]
summary2[1:oneTenth]
storyCorpus = tm_map(storyCorpus,stemDocument)
txt = NULL
for (j in 1:length(storyCorpus)) {
txt = c(txt,storyCorpus[[j]]$content)
}
storySentences2 = txt
dtm = DocumentTermMatrix(storySentences2, minWordLength=4)
tdm = TermDocumentMatrix(storySentences2,control=list(minWordLength=3))
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=4))
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=4))
tdm = removeSparseTerms(tdm,0.999)
tdm = tdm[rowSums(as.matrix(tdm))>0,]
lda.model = LDA(tdm, k)
library(topicmodels)
install.packages("topicmodels")
library(topicmodels)
k = 4
lda.model = LDA(tdm, k)
terms(lda.model,20)
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm = removeSparseTerms(tdm,0.999)
tdm = tdm[rowSums(as.matrix(tdm))>0,]
k = 4
# Beware: this step takes a lot of patience!  My computer was chugging along for probably 10 or so minutes before it completed the LDA here.
lda.model = LDA(tdm, k)
# This enables you to examine the words that make up each topic that was calculated.  Bear in mind that I've chosen to stem all words possible in this corpus, so some of the words output will look a little weird.
terms(lda.model,20)
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm = removeSparseTerms(tdm,0.999)
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm = removeSparseTerms(tdm,0.999)
k = 4
# Beware: this step takes a lot of patience!  My computer was chugging along for probably 10 or so minutes before it completed the LDA here.
lda.model = LDA(tdm, k)
terms(lda.model,20)
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm
k = 3
# Beware: this step takes a lot of patience!  My computer was chugging along for probably 10 or so minutes before it completed the LDA here.
lda.model = LDA(tdm, k)
# This enables you to examine the words that make up each topic that was calculated.  Bear in mind that I've chosen to stem all words possible in this corpus, so some of the words output will look a little weird.
terms(lda.model,20)
library(topicmodels)
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 5
ldaOut <-LDA(tdm,k, method=”Gibbs”, control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut <-LDA(tdm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.topics
ldaOut.terms <- as.matrix(terms(ldaOut,6))
ldaOut.terms
ldaOut
names(ldaOut)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms
?terms
topicProbabilities <- as.data.frame(ldaOut@gamma)
topicProbabilities
storyCorpus
storySentences
storyCorpus = Corpus(VectorSource(storySentences))
storyCorpus = tm_map(storyCorpus,removePunctuation)
storyCorpus = tm_map(storyCorpus,removeWords, stopwords("english"))
storyCorpus = tm_map(storyCorpus,content_transformer(tolower))
storyCorpus = tm_map(storyCorpus,stemDocument)
storyCorpus
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm
storyCorpus[[1]]
storyCorpus[[1]]$content
storyCorpus = tm_map(storyCorpus, stripWhitespace)
storyCorpus
storyCorpus[[1]]$content
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
tdm
ldaOut <-LDA(tdm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms
terms(ldaOut,10)
topicProbabilities <- as.data.frame(ldaOut@gamma)
topicProbabilities
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
topic1ToTopic2
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])
topic1ToTopic2 <- lapply(1:nrow(tdm),function(x)
sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])
topic1ToTopic2
write.csv(ldaOut.terms,file=paste(“LDAGibbs”,k,”TopicsToTerms.csv”))
write.csv(ldaOut.terms,file=paste(“LDAGibbs”,k,”TopicsToTerms.csv”))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))
as.matrix(tdm)
ldaOut <-LDA(tdm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
topicProbabilities <- as.data.frame(ldaOut@gamma)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms
ldaOut.terms <- as.matrix(terms(ldaOut,5))
ldaOut.terms
tdm = TermDocumentMatrix(storyCorpus,control=list(minWordLength=3))
class(tdm)
rowTotals <- apply(tdm, 1, sum)
tdm2 <- dtm[rowTotals>0]
dim(tdm2)
tdm_LDA <- LDA(tdm2, 30)
rowTotals <- apply(tdm, 1, sum)
tdm2 <- tdm[rowTotals>0]
dim(tdm2)
tdm_LDA <- LDA(tdm2, 30)
dim(tdm2)
tdm_LDA <- LDA(tdm2, 30)
tdm2 <- tdm[rowTotals>0]
tdm2
rowTotals <- apply(tdm, 1, sum)
rowTotals
tdm2 <- tdm[rowTotals>0]
tdm2
tdm <- tdm[rowTotals>0]
tdm_LDA <- LDA(tdm,5)
tdmLDA <- LDA(tdm,5)
dtm = DocumentTermMatrix(storyCorpus,control=list(minWordLength=3))
class(dtm)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.topics
ldaOut.terms <- as.matrix(terms(ldaOut,5))
ldaOut.terms
topicProbabilities <- as.data.frame(ldaOut@gamma)
topicProbabilities
dim(dtm)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms
ldaOut.topics
table(ldaout.topics)
table(ldaOut.topics)
getwd()
data(iris)"iris"
data(iris)
pp = names(iris)
pp
pp[2:5]
dim(data)
dim(iris)
dim(iris)[1]
shiny::runApp('C:/Users/Bhushan/Dropbox/R_Projects/SysRisk')
R.Version()
shiny::runApp('C:/Users/Bhushan/Dropbox/R_Projects/SysRisk')
shiny::runApp()
install.packages("igraph")
runApp()
R.Version()
.libPaths()
runApp()
runApp()
shiny::runApp()
